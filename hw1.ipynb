{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 1 1 2 1 2 1 2 2 1 1 2 2 1 1 2 2 2 2 2 2 1 1 1 2 2 1 2 1 2 2 2 1 1 2\n",
      " 2 2 1 1 2 1 2 2 1 2 1 1 1 2 2 1 1 1 2 2 2 1 2 1 1 1 2 2 1 2 1 1 2 1 2 2 2\n",
      " 2 1 2 2 2 1 1 2 1 1 1 2 2 2 1 2 1 2 2 1 2 1 1 2 1 1 2 2 2 1 2 1 2 1 2 2 2\n",
      " 1 2 1 1 1 1 1 2 1 1 2 2 1 2 1 2 2 2 2 2 1 2 2 1 1 2 1 2 1 2 1 2 2 2 2 1 1\n",
      " 1 2 1 2 2 1 1 2 2 1 2 1 2 1 2 1 2 2 1 1 2 2 1 1 2 1 1 2 2 1 1 2 1 2 2 1 1\n",
      " 2 2 2 1 2 2 1 1 1 1 1 2 1 1 2 2 1 2 1 2 1 1 1 2 2 2 1 1 1 1 1 1 1 1 2 1 2\n",
      " 1 1 2 1 2 1 1 1 2 1 1 2 1 1 1 2 1 1 1 1 1 2 1 2 2 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 2 1 2 2 1 2 1 1 2 1 1 1 1 2 1 1 1 2 1 2 2 1 1 2 2 1 2 1 2 2 2 2 1 1 1 2\n",
      " 1 1 1 2]\n",
      "y_truth    1    2\n",
      "y_pred           \n",
      "1        145   14\n",
      "2          5  136\n",
      "y_truth   1   2\n",
      "y_pred         \n",
      "1        48   8\n",
      "2         2  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alpakkanlar/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/alpakkanlar/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#read files\n",
    "\n",
    "data_points = pd.read_csv(\"hw01_data_points.csv\", header = None)\n",
    "class_labels = pd.read_csv(\"hw01_class_labels.csv\",header = None).astype(int)\n",
    "\n",
    "\n",
    "#split data into two parts\n",
    "\n",
    "training_set_data_points = data_points.iloc[0:300]\n",
    "test_set_data_points = data_points.iloc[300:]  \n",
    "\n",
    "training_set_class_labels = class_labels.iloc[0:300]\n",
    "training_set_data_points = np.array(training_set_data_points)\n",
    "\n",
    "\n",
    "training_set_class_labels = np.array(training_set_class_labels)\n",
    "\n",
    "test_set_class_labels =class_labels.iloc[300:]\n",
    "test_set_class_labels = np.array(test_set_class_labels)\n",
    "\n",
    "#############################\n",
    "class_labels = np.array(class_labels)\n",
    "\n",
    "y= training_set_data_points.shape[0]\n",
    "\n",
    "x= np.max(class_labels)\n",
    "\n",
    "\n",
    "#### Estimating the model parameters\n",
    "pAcd = [np.mean(np.logical_and(training_set_class_labels==np.array([c+1]), training_set_data_points == 'A'), axis=0)*2 for c in range(0, 2)]\n",
    "pGcd = [np.mean(np.logical_and(training_set_class_labels==np.array([c+1]), training_set_data_points == 'G'), axis=0)*2 for c in range(0, 2)]\n",
    "pTcd = [np.mean(np.logical_and(training_set_class_labels==np.array([c+1]), training_set_data_points == 'T'), axis=0)*2 for c in range(0, 2)]\n",
    "pCcd = [np.mean(np.logical_and(training_set_class_labels==np.array([c+1]), training_set_data_points == 'C'), axis=0)*2 for c in range(0, 2)]\n",
    "\n",
    "\n",
    "\n",
    "pAcd = np.array(pAcd)\n",
    "pGcd = np.array(pGcd)\n",
    "pCcd = np.array(pCcd)\n",
    "pTcd = np.array(pTcd)\n",
    "\n",
    "############################### Calculating confusion matrix for the training data\n",
    "class_priors = [np.mean(class_labels == (c+1)) for c in range (x)]\n",
    "class_priors = np.log(class_priors)\n",
    "\n",
    "\n",
    "\n",
    "train_data_points_A = ((training_set_data_points == 'A').astype(int))\n",
    "train_data_points_G = ((training_set_data_points == 'G').astype(int))\n",
    "train_data_points_T = ((training_set_data_points == 'T').astype(int))\n",
    "train_data_points_C = ((training_set_data_points == 'C').astype(int))\n",
    "\n",
    "pAcd = np.log(pAcd)\n",
    "pGcd = np.log(pGcd)\n",
    "pTcd = np.log(pTcd)\n",
    "pCcd = np.log(pCcd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sspAcd = np.matmul(train_data_points_A,np.transpose(pAcd))\n",
    "sspGcd = np.matmul(train_data_points_G,np.transpose(pGcd))\n",
    "sspTcd = np.matmul(train_data_points_T,np.transpose(pTcd))\n",
    "sspcCd = np.matmul(train_data_points_C,np.transpose(pCcd))\n",
    "\n",
    "train_sum = np.sum((sspAcd,sspGcd,sspTcd,sspcCd,class_priors), axis = 0)\n",
    "train_sum = np.array(train_sum)\n",
    "\n",
    "train_predicted = (np.argmax(train_sum, axis = 1)) +1\n",
    "print(train_predicted)\n",
    "\n",
    "training_set_class_labels = np.reshape(training_set_class_labels, (300,))\n",
    "\n",
    "confusion_train = pd.crosstab(train_predicted, training_set_class_labels, \n",
    "                               rownames = [\"y_pred\"], \n",
    "                               colnames = [\"y_truth\"])\n",
    "print(confusion_train)\n",
    "\n",
    "\n",
    "############################### Calculating confusion matrix for the test data\n",
    "\n",
    "data_points_A = ((test_set_data_points == 'A').astype(int))\n",
    "data_points_G = ((test_set_data_points == 'G').astype(int))\n",
    "data_points_T = ((test_set_data_points == 'T').astype(int))\n",
    "data_points_C = ((test_set_data_points == 'C').astype(int))\n",
    "\n",
    "\n",
    "\n",
    "spAcd = np.matmul(data_points_A,np.transpose(pAcd))\n",
    "spGcd = np.matmul(data_points_G,np.transpose(pGcd))\n",
    "spTcd = np.matmul(data_points_T,np.transpose(pTcd))\n",
    "spcCd = np.matmul(data_points_C,np.transpose(pCcd))\n",
    "\n",
    "\n",
    "sum = np.sum((spAcd,spGcd,spTcd,spcCd,class_priors), axis = 0)\n",
    "sum = np.array(sum)\n",
    "\n",
    "predicted = (np.argmax(sum, axis = 1)) +1\n",
    "\n",
    "test_set_class_labels = np.reshape(test_set_class_labels, (100,))\n",
    "\n",
    "confusion_test = pd.crosstab(predicted, test_set_class_labels, \n",
    "                               rownames = [\"y_pred\"], \n",
    "                               colnames = [\"y_truth\"])\n",
    "print(confusion_test)\n",
    "\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e6d3936644853644405dcb6c164fe9b5264f1b5dd45dbb4d116b974cc3a302e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
